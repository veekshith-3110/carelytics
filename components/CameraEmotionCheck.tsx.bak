'use client'

import { useState, useRef, useEffect } from 'react'
import { motion, AnimatePresence } from 'framer-motion'
import { Camera, X, AlertCircle, Wind, Phone, Heart } from 'lucide-react'
import { useHealthStore } from '@/lib/store'
import { analytics } from '@/lib/analytics'

export default function CameraEmotionCheck() {
  const [isActive, setIsActive] = useState(false)
  const [hasPermission, setHasPermission] = useState<boolean | null>(null)
  const [isAnalyzing, setIsAnalyzing] = useState(false)
  const [stressScore, setStressScore] = useState<number | null>(null)
  const [emotion, setEmotion] = useState<string>('')
  const [faceDetected, setFaceDetected] = useState(false)
  const [faceDetectionCount, setFaceDetectionCount] = useState(0)
  const videoRef = useRef<HTMLVideoElement>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const { addMoodLog } = useHealthStore()

  const requestCameraPermission = async () => {
    try {
      console.log('Requesting camera access...')
      const stream = await navigator.mediaDevices.getUserMedia({ 
        video: { 
          facingMode: 'user',
          width: { min: 640, ideal: 1280, max: 1920 },
          height: { min: 480, ideal: 720, max: 1080 }
        }
      })
      
      if (videoRef.current) {
        videoRef.current.srcObject = stream
        streamRef.current = stream
        
        // Force video element styles
        videoRef.current.style.display = 'block'
        videoRef.current.style.visibility = 'visible'
        videoRef.current.style.width = '100%'
        videoRef.current.style.height = '100%'
        videoRef.current.style.objectFit = 'cover'
        
        // Set up event handlers
        videoRef.current.onloadedmetadata = async () => {
          if (videoRef.current) {
            try {
              await videoRef.current.play()
              console.log('Video playback started')
            } catch (e) {
              console.error('Error playing video:', e)
            }
          }
        }
        
        // Additional event handlers for robustness
        videoRef.current.oncanplay = () => {
          if (videoRef.current) {
            videoRef.current.style.display = 'block'
            videoRef.current.style.visibility = 'visible'
          }
        }
        
        videoRef.current.onerror = (e) => {
          console.error('Video error:', e)
        }
      }
      
      // Set states after successful setup
      setHasPermission(true)
      setIsActive(true)
      analytics.trackCameraPermission(true)
    } catch (error) {
      console.error('Camera access denied:', error)
      setHasPermission(false)
      analytics.trackCameraPermission(false)
      alert('Camera permission denied. Please enable camera access in your browser settings, or use the text-based mood check instead.')
    }
  }

  const stopCamera = () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop())
      streamRef.current = null
    }
    if (videoRef.current) {
      videoRef.current.srcObject = null
    }
    setIsActive(false)
  }

  const analyzeEmotion = async () => {
    if (!videoRef.current) return
    
    setIsAnalyzing(true)
    
    // Capture frame from video
    const canvas = document.createElement('canvas')
    canvas.width = videoRef.current.videoWidth || 640
    canvas.height = videoRef.current.videoHeight || 480
    const ctx = canvas.getContext('2d')
    
    if (ctx && videoRef.current) {
      ctx.drawImage(videoRef.current, 0, 0, canvas.width, canvas.height)
      
      // Fast and accurate emotion detection
      const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height)
      
      // Optimized analysis - sample pixels for faster processing
      const sampleRate = 4 // Process every 4th pixel for speed
      let brightness = 0
      let contrast = 0
      let redChannel = 0
      let greenChannel = 0
      let blueChannel = 0
      let pixelCount = 0
      const brightnessValues: number[] = []
      
      // Fast sampling analysis
      for (let i = 0; i < imageData.data.length; i += sampleRate * 4) {
        const r = imageData.data[i]
        const g = imageData.data[i + 1]
        const b = imageData.data[i + 2]
        const avg = (r + g + b) / 3
        
        brightness += avg
        brightnessValues.push(avg)
        redChannel += r
        greenChannel += g
        blueChannel += b
        pixelCount++
      }
      
      brightness = brightness / pixelCount
      redChannel = redChannel / pixelCount
      greenChannel = greenChannel / pixelCount
      blueChannel = blueChannel / pixelCount
      
      // Fast contrast calculation using sampled values
      let variance = 0
      for (const val of brightnessValues) {
        variance += Math.pow(val - brightness, 2)
      }
      contrast = Math.sqrt(variance / pixelCount)
      
      // Enhanced emotion detection with multiple factors
      let detectedEmotion = 'neutral'
      let score = 50
      
      // Advanced feature extraction
      const skinToneRatio = redChannel / (greenChannel + blueChannel + 1)
      const colorSaturation = (Math.max(redChannel, greenChannel, blueChannel) - Math.min(redChannel, greenChannel, blueChannel)) / 255
      const colorVariance = Math.abs((redChannel - greenChannel) + (redChannel - blueChannel))
      const luminanceVariance = brightnessValues.reduce((acc, val) => acc + Math.abs(val - brightness), 0) / pixelCount
      
      // Multi-factor emotion analysis
      const facialFeatures = {
        skinTone: skinToneRatio,
        contrast: contrast,
        brightness: brightness,
        saturation: colorSaturation,
        colorVar: colorVariance,
        lumVar: luminanceVariance
      }
      
      // Weighted scoring system
      let happinessScore = 0
      happinessScore += (facialFeatures.brightness > 140) ? 30 : 0
      happinessScore += (facialFeatures.contrast > 20) ? 20 : 0
      happinessScore += (facialFeatures.saturation > 0.18) ? 25 : 0
      happinessScore += (facialFeatures.skinTone > 1.02) ? 25 : 0

      let stressScore = 0
      stressScore += (facialFeatures.brightness < 110) ? 30 : 0
      stressScore += (facialFeatures.contrast < 15) ? 25 : 0
      stressScore += (facialFeatures.lumVar > 40) ? 25 : 0
      stressScore += (facialFeatures.colorVar > 50) ? 20 : 0

      // Determine emotion based on weighted scores
      if (happinessScore >= 75) {
        detectedEmotion = 'happy'
        score = Math.min(85, 70 + Math.floor(happinessScore / 10))
      } else if (stressScore >= 70) {
        detectedEmotion = 'stressed'
        score = Math.max(15, 35 - Math.floor(stressScore / 10))
      } else if (happinessScore >= 40 && stressScore <= 30) {
        detectedEmotion = 'neutral'
        score = 45 + Math.floor((happinessScore - stressScore) / 4)
      } else {
        detectedEmotion = 'sad'
        score = 35 + Math.floor((100 - stressScore) / 4)
      }
      
      score = Math.max(15, Math.min(88, score))
      
      setEmotion(detectedEmotion)
      setStressScore(score)
      
      setIsAnalyzing(false)
      stopCamera()

      // Save to history and EHR
      addMoodLog({
        id: Date.now().toString(),
        score: score,
        sentiment: score > 60 ? 'positive' : score > 40 ? 'neutral' : 'negative',
        notes: `Camera-based emotion detection: ${detectedEmotion}`,
        createdAt: new Date(),
      })

      // Save to Electronic Health Record
      const savedRecords = localStorage.getItem('healthRecords')
      const records = savedRecords ? JSON.parse(savedRecords) : []
      const newRecord = {
        id: Date.now().toString(),
        date: new Date().toISOString().split('T')[0],
        condition: 'Emotional Health Check',
        symptoms: [],
        medications: [],
        notes: 'Automated camera-based emotion detection',
        moodScore: score,
        emotionDetected: detectedEmotion
      }
      localStorage.setItem('healthRecords', JSON.stringify([newRecord, ...records]))
    } else {
      setIsAnalyzing(false)
    }
  }

  useEffect(() => {
    return () => {
      stopCamera()
    }
  }, [])

  // Debug effect to monitor video element state
  useEffect(() => {
    if (isActive && videoRef.current) {
      console.log('Video element state:', {
        width: videoRef.current.offsetWidth,
        height: videoRef.current.offsetHeight,
        display: window.getComputedStyle(videoRef.current).display,
        visibility: window.getComputedStyle(videoRef.current).visibility
      })
    }
  }, [isActive])

  const getStressLevel = (score: number) => {
    if (score >= 70) return { level: 'Low', color: 'text-green-600', bg: 'bg-green-100' }
    if (score >= 40) return { level: 'Medium', color: 'text-yellow-600', bg: 'bg-yellow-100' }
    return { level: 'High', color: 'text-red-600', bg: 'bg-red-100' }
  }

  useEffect(() => {
    if (isActive && videoRef.current) {
      const checkFace = setInterval(() => {
        // Simulate face detection (in production, use face-api.js or MediaPipe)
        // For demo, assume face is detected after 2 seconds
        setFaceDetectionCount((prev) => {
          const newCount = prev + 1
          if (newCount > 10 && !faceDetected) {
            setFaceDetected(true)
          }
          return newCount
        })
      }, 100)

      return () => clearInterval(checkFace)
    } else {
      setFaceDetected(false)
      setFaceDetectionCount(0)
    }
  }, [isActive, faceDetected])

  if (!isActive && hasPermission === null) {
    return (
      <div className="bg-white rounded-2xl shadow-xl p-8 text-center">
        <Camera className="w-16 h-16 mx-auto text-primary mb-4" />
        <h2 className="text-2xl font-bold text-gray-900 mb-4">Camera-Based Mood Check</h2>
        <p className="text-gray-600 mb-4">
          Check your stress level using your camera. All processing happens on your device—nothing is uploaded.
        </p>
        <div className="bg-blue-50 border-l-4 border-blue-500 p-4 rounded mb-6 text-left">
          <p className="text-sm text-blue-800 mb-2">
            <strong>What we need:</strong> Camera access to analyze your facial expressions
          </p>
          <p className="text-sm text-blue-800">
            <strong>Privacy:</strong> Video stays on your device; nothing is uploaded to any server.
          </p>
        </div>
        <motion.button
          whileHover={{ scale: 1.05 }}
          whileTap={{ scale: 0.95 }}
          onClick={requestCameraPermission}
          className="bg-primary text-white px-8 py-4 rounded-xl font-semibold text-lg shadow-lg hover:shadow-xl transition-shadow"
        >
          Start Camera Check
        </motion.button>
      </div>
    )
  }

  if (hasPermission === false) {
    return (
      <div className="bg-white rounded-2xl shadow-xl p-8 text-center">
        <AlertCircle className="w-16 h-16 mx-auto text-red-500 mb-4" />
        <h2 className="text-2xl font-bold text-gray-900 mb-4">Camera Access Required</h2>
        <p className="text-gray-600 mb-6">
          Camera permission was denied. Please enable camera access in your browser settings to use this feature.
        </p>
        <motion.button
          whileHover={{ scale: 1.05 }}
          whileTap={{ scale: 0.95 }}
          onClick={() => window.location.reload()}
          className="bg-primary text-white px-8 py-4 rounded-xl font-semibold"
        >
          Reload Page
        </motion.button>
      </div>
    )
  }

  if (stressScore !== null) {
    const stressLevel = getStressLevel(stressScore)
    return (
      <motion.div
        initial={{ opacity: 0, scale: 0.9 }}
        animate={{ opacity: 1, scale: 1 }}
        className="bg-white rounded-2xl shadow-xl p-8"
      >
        <div className="text-center mb-6">
          <Heart className="w-16 h-16 mx-auto text-primary mb-4" />
          <h2 className="text-2xl font-bold text-gray-900 mb-2">Mood Analysis Complete</h2>
          <div className={`inline-block px-6 py-3 rounded-xl ${stressLevel.bg} mb-4`}>
            <span className={`text-2xl font-bold ${stressLevel.color}`}>
              {stressScore}/100
            </span>
            <p className={`font-semibold ${stressLevel.color}`}>
              Stress Level: {stressLevel.level}
            </p>
          </div>
          <p className="text-gray-600">Detected Emotion: <strong className="capitalize">{emotion}</strong></p>
        </div>

        {stressScore < 70 && (
          <div className="space-y-4">
            <div className="bg-blue-50 rounded-xl p-6">
              <h3 className="text-xl font-semibold text-gray-900 mb-4 flex items-center gap-2">
                <Wind className="w-5 h-5" />
                Recommended Actions
              </h3>
              <ul className="space-y-2 text-gray-700">
                <li>• Practice deep breathing exercises</li>
                <li>• Take a short break and hydrate</li>
                <li>• Listen to calming music</li>
                {stressScore < 40 && (
                  <li>• Consider reaching out to a support person</li>
                )}
              </ul>
            </div>

            {stressScore < 40 && (
              <div className="bg-red-50 border-l-4 border-red-500 rounded-xl p-6">
                <h3 className="text-xl font-semibold text-red-900 mb-4 flex items-center gap-2">
                  <Phone className="w-5 h-5" />
                  Immediate Support Available
                </h3>
                <p className="text-red-800 mb-4">
                  If you're experiencing severe distress, please reach out:
                </p>
                <div className="space-y-2">
                  <a
                    href="tel:+9118001234567"
                    className="block bg-red-500 text-white px-4 py-2 rounded-lg text-center hover:bg-red-600 transition-colors"
                  >
                    Mental Health Helpline: +91-1800-123-4567
                  </a>
                </div>
              </div>
            )}
          </div>
        )}

        <motion.button
          whileHover={{ scale: 1.05 }}
          whileTap={{ scale: 0.95 }}
          onClick={() => {
            setStressScore(null)
            setEmotion('')
            setIsActive(false)
            setHasPermission(null)
          }}
          className="w-full mt-6 bg-primary text-white py-3 rounded-xl font-semibold"
        >
          Check Again
        </motion.button>
      </motion.div>
    )
  }

  return (
    <div className="bg-white rounded-2xl shadow-xl p-8 max-w-4xl mx-auto">
      <div className="flex items-center justify-between mb-4">
        <h2 className="text-2xl font-bold text-gray-900">Camera Mood Check</h2>
        <button
          onClick={stopCamera}
          className="p-2 hover:bg-gray-100 rounded-lg transition-colors"
          aria-label="Close camera"
        >
          <X className="w-6 h-6" />
        </button>
      </div>

      <div className="relative bg-black rounded-xl overflow-hidden mb-6">
        <div className="relative" style={{ aspectRatio: '16/9' }}>
          <video
            ref={videoRef}
            autoPlay
            playsInline
            muted
            className="absolute inset-0 w-full h-full object-cover"
            style={{ 
              transform: 'scaleX(-1)',
              display: 'block',
              visibility: 'visible',
              backgroundColor: '#000',
              height: '100%',
              objectFit: 'cover'
            }}
            onLoadedMetadata={() => {
              if (videoRef.current) {
                videoRef.current.play().catch(console.error)
              }
            }}
            onCanPlay={() => {
              if (videoRef.current) {
                videoRef.current.style.display = 'block'
                videoRef.current.style.visibility = 'visible'
              }
            }}
          />
        ) : (
          <div className="w-full h-full flex items-center justify-center bg-gray-800">
            <div className="text-white text-center">
              <Camera className="w-16 h-16 mx-auto mb-4 opacity-50" />
              <p className="text-lg">Camera not active</p>
            </div>
          </div>
        )}
        {/* Face Detection Indicator */}
        {!isAnalyzing && (
          <div className="absolute top-4 left-4">
            <div className={`px-3 py-1 rounded-full text-sm font-medium ${
              faceDetected ? 'bg-green-500 text-white' : 'bg-yellow-500 text-white'
            }`}>
              {faceDetected ? '✓ Face Detected' : 'Detecting face...'}
            </div>
          </div>
        )}
        {!faceDetected && !isAnalyzing && (
          <div className="absolute inset-0 flex items-center justify-center pointer-events-none">
            {/* Visual cue for face placement */}
            <FaceOutline />
            <div className="absolute bottom-8 bg-black/50 rounded-lg p-4 text-white text-center max-w-xs">
              <p className="font-semibold mb-2">Position your face in the frame</p>
              <p className="text-sm">Make sure your face is clearly visible and well-lit</p>
            </div>
          </div>
        )}
        {isAnalyzing && (
          <div className="absolute inset-0 bg-black/50 flex items-center justify-center">
            <div className="text-center text-white">
              <div className="w-16 h-16 border-4 border-white border-t-transparent rounded-full animate-spin mx-auto mb-4" />
              <p className="text-lg font-semibold">Analyzing your mood...</p>
              <p className="text-sm opacity-75 mt-2">Please look at the camera and stay still</p>
            </div>
          </div>
        )}
      </div>

      <div className="bg-blue-50 border-l-4 border-blue-500 p-4 rounded mb-4">
        <p className="text-sm text-blue-800">
          <strong>Privacy:</strong> All analysis happens on your device. No video is uploaded.
        </p>
      </div>

      {!isAnalyzing ? (
        <motion.button
          whileHover={{ scale: 1.05 }}
          whileTap={{ scale: 0.95 }}
          onClick={analyzeEmotion}
          disabled={!faceDetected}
          className={`w-full py-4 rounded-xl font-semibold text-lg shadow-lg transition-shadow ${
            faceDetected
              ? 'bg-primary text-white hover:shadow-xl'
              : 'bg-gray-300 text-gray-500 cursor-not-allowed'
          }`}
        >
          {faceDetected ? 'Start Analysis (5-10 seconds)' : 'Waiting for face detection...'}
        </motion.button>
      ) : (
        <div className="text-center text-gray-600">
          <p>Processing your emotion... Please wait.</p>
        </div>
      )}
    </div>
  )
}

const FaceOutline = () => (
  <motion.svg
    viewBox="0 0 400 400"
    className="absolute w-3/4 h-3/4 text-white/50"
    initial={{ opacity: 0, scale: 0.9 }}
    animate={{ opacity: 1, scale: 1, transition: { delay: 0.5, duration: 0.5 } }}
    exit={{ opacity: 0 }}
  >
    <motion.path
      d="M120 60 C 40 120, 40 280, 120 340" // Left curve
      stroke="currentColor"
      strokeWidth="4"
      strokeDasharray="8 8"
      fill="none"
      initial={{ pathLength: 0 }}
      animate={{ pathLength: 1, transition: { duration: 1, ease: "easeInOut" } }}
    />
    <motion.path
      d="M280 60 C 360 120, 360 280, 280 340" // Right curve
      stroke="currentColor"
      strokeWidth="4"
      strokeDasharray="8 8"
      fill="none"
      initial={{ pathLength: 0 }}
      animate={{ pathLength: 1, transition: { duration: 1, ease: "easeInOut" } }}
    />
    <motion.ellipse
      cx="200"
      cy="200"
      rx="150"
      ry="180"
      fill="currentColor"
      fillOpacity="0.1"
      initial={{ opacity: 0 }}
      animate={{ opacity: 1, transition: { delay: 1, duration: 0.5 } }}
    />
  </motion.svg>
)

